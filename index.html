<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction">
  <meta name="keywords" content="Neural Radiance Field, Scene Reconstruction, Prior">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MonoInstance</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://wen-yuan-zhang.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://wen-yuan-zhang.github.io/Fast-Learning-NeRF/">
            Fast-Learning NeRF
          </a>
          <a class="navbar-item" href="https://wen-yuan-zhang.github.io/VolumeRenderingPriors/">
            VolumeRenderingPriors
          </a>
          <a class="navbar-item" href="https://wen-yuan-zhang.github.io/GS-Pull/">
            GS-Pull
          </a>
          <a class="navbar-item" href="https://wen-yuan-zhang.github.io/NeRFPrior/">
            NeRFPrior
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction</h1>
          <div class="column is-full_width">
            <h2 class="title is-4">CVPR 2025</h2>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://wen-yuan-zhang.github.io/">Wenyuan Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              Yixiao Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/alvin528">Han Huang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/hanl2010">Liang Han</a><sup>1</sup>,
            </span>
            <span class="author-block">
              Kanle Shi<sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="https://h312h.github.io/">Zhizhong Han</a><sup>3</sup>
            </span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>School of Software, Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Kuaishou Technology</span>
            <span class="author-block"><sup>3</sup>Wayne State University</span>
          </div>

          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/wen-yuan-zhang/MonoInstance"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -30px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Monocular depth priors have been widely adopted by neural rendering in multi-view based tasks such as 3D reconstruction and novel view synthesis. However, due to the inconsistent prediction on each view, how to more effectively leverage monocular cues in a multi-view context remains a challenge. Current methods treat the entire estimated depth map indiscriminately, and use it as ground truth supervision, while ignoring the inherent inaccuracy and cross-view inconsistency in monocular priors. To resolve these issues, we propose MonoInstance, a general approach that explores the uncertainty of monocular depths to provide enhanced geometric priors for neural rendering and reconstruction. Our key insight lies in aligning each segmented instance depths from multiple views within a common 3D space, thereby casting the uncertainty estimation of monocular depths into a density measure within noisy point clouds. For high-uncertainty areas where depth priors are unreliable, we further introduce a constraint term that encourages the projected instances to align with corresponding instance masks on nearby views. MonoInstance is a versatile strategy which can be seamlessly integrated into various multi-view neural rendering frameworks. Our experimental results demonstrate that MonoInstance significantly improves the performance in both reconstruction and novel view synthesis under various benchmarks.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Method</h2>
        <img src="./resources/method.png" class="center">
        <div class="content has-text-justified">
          <p>
            In this paper, we introduce <b>MonoInstance</b>, which detects uncertainty in 3D according to inconsistent clues from monocular priors on multi-view. 
            Our method is a general strategy to enhance monocular priors for various multi-view neural rendering and reconstruction frameworks.
            Based on the uncertainty maps, we introduce novel strategies to reduce the negative impact brought by inconsistent monocular clues and mine more reliable supervision through photometric consistency.
          </p>
          <p style="margin-top: 30px">
            Here is an overview of our method. We take multi-view 3D reconstruction through NeRF based rendering as an example.
            <strong>(a)</strong> Starting from multi-view consistent instance segmentation and estimated monocular depths, we align the same instance from different viewpoints by back-projecting instance depths into a point cloud. 
            The monocular inconsistent clues across different views become a measurement of density estimation in neighborhood of each point, leading to uncertainty maps (Section 3.2). 
            The estimated uncertainty maps are further utilized in <strong>(b)</strong> neural rendering pipeline to guide adaptive depth loss, ray sampling (Section 3.4) and 
            <strong>(c)</strong> instance mask constraints (Section 3.3).            
          </p>
        </div>

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Visualization Results</h2>


        <h3 class="title is-4">Experiments on Dense-view Reconstruction Task</h3>
        <div class="content has-text-justified">
          <img src="./resources/scannet.png" class="center">
          <img src="./resources/scannet2.png" class="center">
          <p>
            We compare our method with the latest indoor scene reconstruction methods using dense viewpoints on ScanNet and Replica datasets.
          </p>
        

        <h3 class="title is-4">Experiments on Sparse-view Reconstruction Task</h3>
        </div>
        <div class="content has-text-justified">
          <img src="./resources/dtu.png" class="center">
          <img src="./resources/dtu2.png" class="center">
          <p>
            We also evaluate our method in reconstructing 3D shapes from sparse observations on DTU dataset. 
            Each of the scene shows single object with background from 3 viewpoints with small overlapping. 
          </p>
        </div>

        <h3 class="title is-4">Experiments on Sparse Novel View Synthesis Task</h3>
        <div class="content has-text-justified">
          <img src="./resources/llff.png" class="center">
          <img src="./resources/llff2.png" class="center">
          <p>
            We further evaluate our method on 3DGS-based sparse-input novel view synthesis (NVS) task on LLFF dataset.  
            Each on of the forward-facing real-world scenes contains 3 views. 
            In the uncertainty maps, areas that are more white indicate higher uncertainty.
          </p>
        </div>


          
        

        <h3 class="title is-4">Scene Display And More Visualization Results</h3>
        <div class="content has-text-justified">
          <p>
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="90%">
            <source src="./resources/video.mp4"
                    type="video/mp4">
          </video>
        </div>
        

      </div>
    </div>

  </div>
</section>

</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
     <pre><code>@inproceedings{zhang2025monoinstance,
      title={MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction},
      author={Wenyuan Zhang and Yixiao Yang and Han Huang and Liang Han and Kanle Shi and Yu-Shen Liu and Zhizhong Han},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
      year={2025}
  }
</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
